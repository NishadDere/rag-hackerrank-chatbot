A Retrieval-Augmented Generation (RAG) chatbot built using:

ChromaDB (Vector database)

MPNet embeddings (all-mpnet-base-v2)

BGE Reranker (BAAI/bge-reranker-base)

Groq LLM (llama-3.1-8b-instant)

Conversational memory

Strict & Hybrid answer modes

Typing animation + chunk previews + confidence scoring

This chatbot answers questions by retrieving the most relevant sections (â€œchunksâ€) from a HackerRank-style document and generating answers with citations that trace back to the source text.

âœ¨ Features
ğŸ” 1. Retrieval-Augmented Generation (RAG)

Document â†’ chunking â†’ embeddings â†’ stored in ChromaDB

Multi-query expansion improves retrieval recall

BGE reranker improves ranking relevance

Provenance: each answer links back to exact text chunks

ğŸ­ 2. Answer Modes

Strict Mode â†’ Only uses document context

Hybrid Mode â†’ Mixes document + model knowledge with disclaimer

Switch anytime using:

/mode strict
/mode hybrid

ğŸ’¬ 3. Conversational Memory

Keeps last 4 conversation turns

Allows follow-up questions like:
â€œExplain it in simple words.â€
â€œGive an example.â€

ğŸ” 4. Chunk Previews

Enable:

/preview on


Shows where the answer came from.

ğŸ“š 5. Citations

Enable/disable:

/citations on
/citations off

ğŸ§  6. Confidence Score

Each answer returns a 0.0â€“1.0 confidence value based on context coverage.

ğŸ¨ 7. ChatGPT-style typing animation

Realistic type-writer effect in console.

ğŸ“ Project Structure
rag-hackerrank-chatbot/
â”‚
â”œâ”€â”€ code/
â”‚   â”œâ”€â”€ ingest_and_chunk.py
â”‚   â”œâ”€â”€ embed_chunks.py
â”‚   â”œâ”€â”€ index_chroma.py
â”‚   â”œâ”€â”€ retriever_chroma.py
â”‚   â”œâ”€â”€ answer_with_provenance.py
â”‚   â”œâ”€â”€ chatbot.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ hackerrank_doc.txt
â”‚
â”œâ”€â”€ chroma_db/            # ignored
â”œâ”€â”€ venv/                 # ignored
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

âš™ï¸ Installation
1. Clone the repository
git clone https://github.com/NishadDere/rag-hackerrank-chatbot.git
cd rag-hackerrank-chatbot

2. Create a virtual environment
python -m venv venv
venv\Scripts\activate   # Windows

3. Install dependencies
pip install -r requirements.txt


If you donâ€™t have requirements.txt, generate it:

pip freeze > requirements.txt

4. Add your Groq API key

Create a .env file:

GROQ_API_KEY=your_key_here
GROQ_MODEL=llama-3.1-8b-instant

ğŸ—ï¸ Data Processing & Indexing
Step 1 â€” Chunk the document
python -m code.ingest_and_chunk

Step 2 â€” Create embeddings
python -m code.embed_chunks

Step 3 â€” Index into ChromaDB
python -m code.index_chroma

ğŸ¤– Running the Chatbot
python -m code.chatbot

Example Commands:
/mode hybrid
/citations off
/preview on

Example Questions:
What is regression?
Explain in simple words.
Is regression supervised or unsupervised?
What are the steps of KNN?

ğŸ§ª Example Output (Strict Mode)
Bot: Regression predicts continuous values. [Chunk 0]
Regression models relationships between variables. [Chunk 2]
Confidence: 88%

ğŸ›¡ï¸ .gitignore Summary

This project safely ignores:

venv/

chroma_db/

.env

*.pkl

__pycache__/

So no API keys or local DB data are ever uploaded to GitHub.

ğŸ”® Future Improvements

Web UI (FastAPI + React or Streamlit Support)

Better memory summarization

UI components for chunk previews

Evaluation metrics for retrieval quality

PDF/document ingestion

Fine-tuned domain models